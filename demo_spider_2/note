一、requests模块
    requests模块
      -urllib模块，古老复杂
      -requests模块，简洁

    python中原生的一款基于网络请求的模块，功能强大，简单便捷，效率极高。
    作用：模拟浏览器发请求。

    如何使用：（requests模块的编码流程）
        -指定url
        -发起请求
        -获取响应数据
        -持久化存储

    环境安装：
        pip install requests

    实战编码：
        -需求:
            -爬取搜狗首页数据（设计一个简易的网页采集器）
                -UA监测
                -UA伪装
            -破解百度翻译
                -post请求（携带了参数）
                -响应数据是一组json数据
            -爬取豆瓣电影分类排行榜中电影数据的详情
            -爬取肯德基餐厅查询中指定地点的餐厅数量位置信息
            -爬取国家药监局中基于中华人民共和国化妆品生产许可证相关数据
                -动态加载数据
                -首页中对应的数据是通过ajax请求到的
                -通过对详情页url的观察，域名都一样，只有携带的参数不一样，id的值可以从首页对应的ajax请求到的json串中获取，域名和id值拼接出一个完整的企业对应的详情页url
                -详情页信息也是动态加载的
                    -所有post请求的url是一样的，只有参数值不同

聚焦爬虫：爬取页面中指定的页面内容。
    -编码流程：
        -指定url
        -发起请求
        -获取响应数据
        -数据解析
        -持久化存储


二、数据解析
    数据解析原理概述：
        -解析的局部文本内容都会在标签之间或者标签对应的属性中进行存储
        -进行指定标签的定位
        -标签或者标签对应的属性中存储的数据进行提取（解析）
    数据解析分类：
        -正则
        -bs4
            -实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中
                -from bs4 import BeautifulSoup
                -对象的实例化
                    -1.将本地的html文档中的数据加载到该对象中
                        fp = open('./test.html','r',encoding = 'utf-8')
                        soup  = BeautifulSoup(fp,'lxml')
                    -2.将互联网中获取到的页面源码加载到该对象中
                        page_text = response.text
                        soup = BeautifulSoup(page_text,'lxml')
                -soup.tagName:返回文档中第一次出现的tagName对应的标签
                -soup.find()
                    -soup.find('tagName') 相当于soup.tagName
                    -属性定位：soup.find('div',class_ = 'tags list')
                -soup.find_all('tagName'):返回符合要求的所有标签（列表）
                -select:
                    -select('某种选择器（id，class，标签...选择器）')，返回的是一个列表
                    -层级选择器：soup.select('.tagslist > ul  a')  '>'表示一个层级，空格表示多个层级
                -获取标签之间的文本数据：
                    -soup.a.text/string/get_text()
                    -text和get_text()可以获取某个标签中所有的文本内容，string只可以获取该标签下直系的文本内容
                -获取标签中的属性值：
                    -soup.a['href']
            -通过调用BeautifulSoup对象中的相关属性或者方法进行标签定位和数据提取
        -xpath：最常用且最便捷高效，通用性最强的解析方式
            -解析原理：
                -1、实例化一个etree的对象，并将需要解析的页面源码加载到该对象中
                -2、调用etree对象中的xpath方法结合xpath表达式实现标签的定位和内容的捕获
            -环境的安装：
                -pip install lxml
            -如何实例化一个etree对象:from lxml import etree
                -将本地的html文档中的源码数据加载到etree对象中
                    etree.parse(filePath)
                -将互联网上获取的源码数据加载到该对象中
                    etree.HTML('page_text')
                -xpath('xpath表达式 ')
            -xpath表达式：
                -/：表示从根节点开始定位，表示的是一个层级
                -//:表示多个层级，表示从任意位置开始定位
                -属性定位：//tag[@attrName = "attrValue"]
                -索引定位：//tag[@attrName = "attrValue"]/tag[num]  索引从1开始
                -取文本
                    -/text() 获取标签中直系的文本内容
                    -//text()  获取标签中非直系的文本内容（所有文本内容）
                -取属性
                    -/@attrName


三、异步爬虫
    目的：在爬虫中使用异步实现高性能的数据爬取操作。
    异步爬虫的方式：
        -多线程，多进程（不建议）
            好处：可以为相关阻塞的操作单独开启线程或进程，阻塞操作就可以异步执行。
            弊端：无法无限制的开启多线程或多进程。
        -线程池、进程池（适当使用）
            好处：降低系统对进程或者线程创建和销毁的频率，从而降低系统的开销。
            弊端：池中线程或进程数量有上限。
        -单线程+异步协程（推荐）
            -event_loop：事件循环，相当于一个无限循环，可以把一些函数注册到这个事件循环上，当满足某些条件的时候，函数就会被循环执行
            -coroutine:协程对象，将协程对象注册到事件循环中，它会被事件调用，可以使用async关键字定义一个方法，这个方法在调用是不会立即被执行，而是返回一个协程对象
            -task：任务，对协程对象的进一步封装，包含了任务的各个状态
            -future：代表将来执行或还没有执行的任务，实际上和task没有本质区别
            -async：定义一个协程
            -await：用来挂起阻塞方法的执行

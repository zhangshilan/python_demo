requests模块
  -urllib模块，古老复杂
  -requests模块，简洁

python中原生的一款基于网络请求的模块，功能强大，简单便捷，效率极高。
作用：模拟浏览器发请求。

如何使用：（requests模块的编码流程）
    -指定url
    -发起请求
    -获取响应数据
    -持久化存储

环境安装：
    pip install requests

实战编码：
    -需求:
        -爬取搜狗首页数据（设计一个简易的网页采集器）
            -UA监测
            -UA伪装
        -破解百度翻译
            -post请求（携带了参数）
            -响应数据是一组json数据
        -爬取豆瓣电影分类排行榜中电影数据的详情
        -爬取肯德基餐厅查询中指定地点的餐厅数量位置信息
        -爬取国家药监局中基于中华人民共和国化妆品生产许可证相关数据
            -动态加载数据
            -首页中对应的数据是通过ajax请求到的
            -通过对详情页url的观察，域名都一样，只有携带的参数不一样，id的值可以从首页对应的ajax请求到的json串中获取，域名和id值拼接出一个完整的企业对应的详情页url
            -详情页信息也是动态加载的
                -所有post请求的url是一样的，只有参数值不同



聚焦爬虫：爬取页面中指定的页面内容。
    -编码流程：
        -指定url
        -发起请求
        -获取响应数据
        -数据解析
        -持久化存储
数据解析分类：
    -正则
    -bs4
        -实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中
            -from bs4 import BeautifulSoup
            -对象的实例化
                -1.将本地的html文档中的数据加载到该对象中
                    fp = open('./test.html','r',encoding = 'utf-8')
                    soup  = BeautifulSoup(fp,'lxml')
                -2.将互联网中获取到的页面源码加载到该对象中
                    page_text = response.text
                    soup = BeautifulSoup(page_text,'lxml')
            -soup.tagName:返回文档中第一次出现的tagName对应的标签
            -soup.find()
                -soup.find('tagName') 相当于soup.tagName
                -属性定位：soup.find('div',class_ = 'tags list')
            -soup.find_all('tagName'):返回符合要求的所有标签（列表）
            -select:
                -select('某种选择器（id，class，标签...选择器）')，返回的是一个列表
                -层级选择器：soup.select('.tagslist > ul  a')  '>'表示一个层级，空格表示多个层级
            -获取标签之间的文本数据：
                -soup.a.text/string/get_text()
                -text和get_text()可以获取某个标签中所有的文本内容，string只可以获取该标签下直系的文本内容
            -获取标签中的属性值：
                -soup.a['href']
        -通过调用BeautifulSoup对象中的相关属性或者方法进行标签定位和数据提取
    -xpath
数据解析原理概述：
    -解析的局部文本内容都会在标签之间或者标签对应的属性中进行存储
    -进行指定标签的定位
    -标签或者标签对应的属性中存储的数据进行提取（解析）


scrapy框架
-什么是框架？
    -集成了很多功能的具有很强通用性的项目模板。
-如何学习框架？
    -专门学习框架封装的各种功能的详细用法。
-什么是scrapy？
    -爬虫中封装好的明星框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式。

-scrapy框架的基本使用
    -环境的安装：
        -wheel
        -twisted
        -pywin32
        -scrapy
    -创建你一个工程：scrapy startproject xxxPro
    -cd xxxPro
    -在spiders子目录中创建一个爬虫文件
        - scrapy genspider spiderName www.xxx.com
    -执行工程：
        -scrapy crawl spiderName
    -修改robotstxt，进行UA伪装
-scrapy数据解析

-scrapy持久化存储
    -基于终端指令：
        -要求：只可以将parse方法的返回值存储到本地文本文件中
        -注意：对应的文本文件的类型只能为：json,jsonlines,jl,csv,xml,marshal,pickle
        -指令：scrapy crawl xxx -o filePath
        -优点：简洁高效快捷；缺点：局限性强，只能将数据存储到指定后缀的文本文件中
    -基于管道：
        -编码流程：
            -数据解析
            -在item类中定义相关的属性
            -将解析的数据封装存储到item类型的对象中
            -将item类型的对象提交给管道进行持久化存储的操作
            -在管道类的procecss_item中要将其接受到的item对象中存储的数据进行持久化存储操作
            -在配置文件中开启管道
        -通用性强

-基于spider的全站数据爬取
    -就是将网站中某板块下的全部页码对应的页面数据进行爬取和解析
    -手动请求发送：
        -yield scrapy.Request(url,callback):callback专门用作于数据解析
-五大核心组件
    -引擎（scrapy）：用来处理整个系统的数据流，触发事务（框架核心）
    -调度器（scheduler）：用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。可以想象成一个url的优先队列，由他来决定下一个抓取的网址是什么，同时去除重复的网址
    -下载器（downloader）：用于下载网页内容，并将网页内容返回给spider（建立在twisted这个高效的异步模型上）
    -爬虫（spider）：用于从特定的网页中提取自己需要的信息，即所谓的实体（item），用户也可以从中取出链接，让scrapy抓取下一个页面
    -项目管道（pipeline）：负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息，当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据

-请求传参
    -使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
    -需求：爬取boss直聘的岗位名称和岗位详情
scrapy框架
-什么是框架？
    -集成了很多功能的具有很强通用性的项目模板。
-如何学习框架？
    -专门学习框架封装的各种功能的详细用法。
-什么是scrapy？
    -爬虫中封装好的明星框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式。

-scrapy框架的基本使用
    -环境的安装：
        -wheel
        -twisted
        -pywin32
        -scrapy
    -创建你一个工程：scrapy startproject xxxPro
    -cd xxxPro
    -在spiders子目录中创建一个爬虫文件
        - scrapy genspider spiderName www.xxx.com
    -执行工程：
        -scrapy crawl spiderName
    -修改robotstxt，进行UA伪装
-scrapy数据解析

-scrapy持久化存储
    -基于终端指令：
        -要求：只可以将parse方法的返回值存储到本地文本文件中
        -注意：对应的文本文件的类型只能为：json,jsonlines,jl,csv,xml,marshal,pickle
        -指令：scrapy crawl xxx -o filePath
        -优点：简洁高效快捷；缺点：局限性强，只能将数据存储到指定后缀的文本文件中
    -基于管道：
        -编码流程：
            -数据解析
            -在item类中定义相关的属性
            -将解析的数据封装存储到item类型的对象中
            -将item类型的对象提交给管道进行持久化存储的操作
            -在管道类的procecss_item中要将其接受到的item对象中存储的数据进行持久化存储操作
            -在配置文件中开启管道
        -通用性强

-基于spider的全站数据爬取
    -就是将网站中某板块下的全部页码对应的页面数据进行爬取和解析
    -手动请求发送：
        -yield scrapy.Request(url,callback):callback专门用作于数据解析
-五大核心组件
    -引擎（scrapy）：用来处理整个系统的数据流，触发事务（框架核心）
    -调度器（scheduler）：用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。可以想象成一个url的优先队列，由他来决定下一个抓取的网址是什么，同时去除重复的网址
    -下载器（downloader）：用于下载网页内容，并将网页内容返回给spider（建立在twisted这个高效的异步模型上）
    -爬虫（spider）：用于从特定的网页中提取自己需要的信息，即所谓的实体（item），用户也可以从中取出链接，让scrapy抓取下一个页面
    -项目管道（pipeline）：负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息，当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据

-请求传参(未完成，boss直聘网站更改)
    -使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
    -需求：爬取boss直聘的岗位名称和岗位详情

-图片数据爬取之ImagesPipeline:
    -基于scrapy爬取字符串类型的数据和爬取图片类型的数据有什么区别?
        -字符串：只需要基于xpath进行解析并提交管道进行持久化存储
        -图片：xpath解析出图片的src属性值。单独对图片地址发起请求获取图片二进制类型的数据

    -ImagesPipeline
        -只需要将img的src属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型的数据
    -使用流程：
        -数据解析（图片的地址）
        -将存储图片地址的item提交到指定的管道类
        -在管道文件中自制定一个基于ImagesPipeline的管道类
            -get_media_request()
            -file_path()
            -item_completed()
        -早配置文件中指定图片的存储目录以及开启的管道
            -IMAGES_STORE = './XXX'

-中间件
    -下载中间件
        -位置：引擎和下载器之间
        -作用：批量拦截到整个工程中所有的请求和响应
        -拦截请求：（http代理有问题，没有运行成功）
            -UA伪装:process_request
            -代理IP:process_exception:return request
        -拦截响应：
            -篡改响应数据，响应对象
            -需求：爬取网易新闻中的新闻数据（标题和内容）
                -1.通过网易新闻的首页解析出五大板块对应详情页的url（无动态加载）
                -2.每一个板块对应的新闻标题都是动态加载出来的（动态加载）
                -3.通过解析出每一条新闻详情页的url获取详情页的页面源码，解析出新闻的内容